# Общие подходы для первоначального извлечения документов
## Разреженное извлечение
Ранжирование основано на лексическом соотвествии, не использует сторонние обученные модели.
### BM25 (+ WAND) 
Без алгоритма обрезания мы ранжируем сразу по всему, что на больших данных занимает слишком много времени. \
С высокой вероятностью мы упустим нужный источник.
### Динамический WAND
[Может быть полезно](https://www.researchgate.net/publication/221613425_Efficient_query_evaluation_using_a_two-level_retrieval_process)\
Согласно статье, этот метод работает кратно быстрее обычного BM25 и практически без потери точности. \ 

0.16 @rank10.
[Пример](https://github.com/vespa-engine/sample-apps/tree/master/transformers)
## Плотное извлечение с помощью Transformer
Общая идея: Документы переводим в векторное пространство BERT-подобными моделями, 
затем ищем ближайших соседей с помощью HNSW.
Среди актуальных, понятной оценкой и примерами использования с vespa нашел следующие два:
- [основанная на MiniLM](https://huggingface.co/sentence-transformers/msmarco-MiniLM-L-6-v3). \
    Сложнее интегрировать, 0.30 @rank10.
- [E5-small-v2](https://huggingface.co/intfloat/e5-small-v2)
    vespa указывает на ее эффективность при работе с большими объемами документов, но конкретного сравнения я не нашел
- ColBERT \
    Интегрирована в vespa. Показывает наилучший результат 0.35 @rank10.

## ColPali
[Ссылка 1](https://blog.vespa.ai/the-rise-of-vision-driven-document-retrieval-for-rag/),\
[Ссылка 2](https://medium.com/the-ai-forum/implement-multimodal-rag-with-colpali-and-vision-language-model-groq-llava-and-qwen2-vl-5c113b8c08fd). \
Если плотные подходы основаны на BERT-подобных моделях, то ColPaLi реализует подход индексации по визуальным признакам. \
Ориентирована на работу с pdf-подобными документами \ 
В самой документации Vespa пока ничего не написано про интегрирование этой модели, 
и поэтому необходимо еще изучать как мы сможем ее использовать. \
[Пример использования](https://github.com/vespa-engine/sample-apps/tree/master/visual-retrieval-colpali)


# Способы дополнительного отбора, а именно ранжирования документов
### BM25
Данную функцию можно использовать и тут засчет своей точности, и высокой производительности на малом объеме данных
### nativeRank
Лучшая функция из встроенных функций ранжирования, работает точнее BM25, но скорость в 3-4 раза ниже
### Моделями ранжирования(cross-encoder)
Нет точной информации по сравнению с nativeRank, но предположитель работает точнее \
Кратно повышает требуемые мощности, для весомого результата требует тонкой настройки
По результатам тестирования с colbert выдал: 0.39 @rank10.(с моделью)

### Примеры
- [Пример: bm25 + SPLADE](https://github.com/vespa-engine/sample-apps/tree/master/visual-retrieval-colpali)
# Более подробные описания алгоритмов/функций
### BM25
Функция ранжирования для оценки релевантности текстового документа по поисковому запросу \
Запрос разбивается на термины, а далее они сопоставляются с индексированием документа
[Далее идет математика](https://docs.vespa.ai/en/reference/bm25.html)

### nativeRank
Является наиболее точной из функций ранжирования. Является линейной комбнацией по трем критериями:
- nativeFieldMatch. Смотрит как термины запроса сопоставляются с терминами документа
- nativeProximity. Фиксирует, насколько близко встречаются совпадающие термины запроса в полях индекса
- nativeAttributeMatch. Смотрит, насколько хорошо термины запроса соответствуют искомым атрибутивным полям(идейно как именно мы это понимаем не объясняется, просто как эффективная магия с весами)
Идеи каждой из трех метрик может использоваться как по отдельности, так и в других метриках/алгоритмах, но самостоятельное применение получила лишь первая, которая является некоторым аналогом bm25.


### WAND
В vespa есть два запроса, с помощью которых выполняется данный алгоритм: weakAnd и wand.
- weakAnd \
  Принимает термины запроса. Извлекает документы по частоте для каждого термина из запроса, не более порогового значения. Термин значимости определяется в запросе, но зачастую использует формула основаннная на [TF-IDF - обратная частоста документа](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) \
  (Динамическая модицикация) weakAnd полагается на обратную связь, определяющую, какие попадания используются для ранжирования первой фазы, чтобы изменить порог.
- wand
  Принимает веса или признаки документа, которые уже самостоятельно переводит в веса.  \
  Ранжирует по скалярному произведению между вектором запроса и вектором документа и возвращает топ-k (так как работает уже с результатом weekAnd, мы получаем приемлимую скорость запроса) \
  
[Подробнее](https://docs.vespa.ai/en/using-wand-with-vespa.html)

### HNSW
Алгоритм приблизительного поиск ближайшего соседа. Кратно повышает производительность по сравнению с kNN, а в силу дополнительного ранжирования, ошибки в точности почти не влияют на итоговый результат
[Имеет различные опции ускорения при помощи инструментов vespa](https://docs.vespa.ai/en/approximate-nn-hnsw.html)


# Дополнительно
### Использования русского языка
По умолчанию, в vespa используется библиотека OpenNLP для токенизации текста. Она в том числе поддерживает русский и еще 20 языков. \
Также vespa дает понятные инструменты для поддерживания другой библиотеки - Lucene. Она поддерживает куда больше языков, но не умеет самостоятельно определять язык. 

Более подробно их сравнение было описано в статье: \
[Сравнение моделей для русского языка + пример](https://habr.com/ru/companies/sportmaster_lab/articles/848992/)  

Коротко, автор указал, что Lucene более корректно токенизирует русский язык, что позволяет точнее ранжировать документы 

Другие ссылки по этому вопросу: \
[Статья от vespa про многоязычные модели](https://blog.vespa.ai/simplify-search-with-multilingual-embeddings/) \ 
[Про использования Lucene из документации](https://docs.vespa.ai/en/lucene-linguistics.html)





### Про методы оценки
При оценивании обычно используют метрику @rank10, которая определяет, есть ли найденный алгоритмом документ в топ-10 самых релевантных. \ 
Если же смотреть на метрики попадания в топ-100 или топ-1000, они растут экспоненциально, и к топ-1000 все предложенные методы выдают примерно 0,9 точности. Надо понимать, что метрики подходов были получены на датасете MS Marco с сотнями тысяч документов, и совсем не очевидно, что они сохранят соотношение точности в условиях вашей задачи.

### Ссылки по теме:

- https://habr.com/ru/articles/545634/
- https://arxiv.org/abs/2104.08663
- https://blog.google/products/search/search-language-understanding-bert/
- https://bergum.medium.com/how-not-to-use-bert-for-search-ranking-4586716428d9